{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/liuzitian/coding/CSP/Week 1/Day 3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuzitian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# on first time run\n",
    "#!pip install --user scikit-learn\n",
    "#!pip install --user nltk\n",
    "\n",
    "# import packages and stopwords list\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# use English stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# read the data - replace with your own data\n",
    "df = pd.read_csv(\"trump_insult_tweets_2014_to_2021.csv\")\n",
    "\n",
    "# drop data with missing values in the 'Review' column\n",
    "df = df.dropna(axis=0, subset=['tweet'])\n",
    "\n",
    "# convert the relevant column to lowercase\n",
    "df['tweet'] = df.tweet.str.lower() \n",
    "\n",
    "# create a list of the reviews from the 'Reviews' column\n",
    "words = df['tweet'].tolist()\n",
    "\n",
    "# tokenise the words\n",
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "word_tokens = []\n",
    "for review in words:\n",
    "    word_tokens.append(word_tokenize(review))  \n",
    "\n",
    "# create a new list with stop words removed\n",
    "filtered_reviews = []\n",
    "for review in word_tokens:\n",
    "    filtered_reviews.append([w for w in review if not w in stops])\n",
    "\n",
    "# add the tokens to the dataframe    \n",
    "df['tokens'] = filtered_reviews\n",
    "\n",
    "# stem the tokens\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "stemmed = []\n",
    "for review in filtered_reviews: \n",
    "    stemmed.append([ps.stem(w) for w in review])\n",
    "    \n",
    "# add to the dataframe\n",
    "df['tokens'] = stemmed\n",
    "\n",
    "# put the tokens back together as text\n",
    "import string\n",
    "rejoin = []\n",
    "for review in stemmed:\n",
    "    x = \",\".join(review) # join the text back together \n",
    "    x = x.replace(\",\", \" \") # replace commas with spaces\n",
    "    # remove punctuation from the reviews using the string package\n",
    "    rejoin.append(x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# add the reformed text to the data frame    \n",
    "df['filtered_tweet'] = rejoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>fool</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>[believ, fool, ,, dr., thoma, frieden, cdc, ,,...</td>\n",
       "      <td>believ fool   dr thoma frieden cdc   state    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>DOPE</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>[believ, fool, ,, dr., thoma, frieden, cdc, ,,...</td>\n",
       "      <td>believ fool   dr thoma frieden cdc   state    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>politicians</td>\n",
       "      <td>all talk and no action</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>[big, time, u.s., today, -, make, america, gre...</td>\n",
       "      <td>big time us today  make america great  politic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>ben-cardin</td>\n",
       "      <td>It's politicians like Cardin that have destroy...</td>\n",
       "      <td>politician @senatorcardin didn't like that i s...</td>\n",
       "      <td>[politician, @, senatorcardin, n't, like, said...</td>\n",
       "      <td>politician  senatorcardin nt like said baltimo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>neil-young</td>\n",
       "      <td>total hypocrite</td>\n",
       "      <td>for the nonbeliever, here is a photo of @neily...</td>\n",
       "      <td>[nonbeliev, ,, photo, @, neilyoung, offic, $, ...</td>\n",
       "      <td>nonbeliev   photo  neilyoung offic   request—t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date          target  \\\n",
       "0           1  2014-10-09  thomas-frieden   \n",
       "1           2  2014-10-09  thomas-frieden   \n",
       "2           3  2015-06-16     politicians   \n",
       "3           4  2015-06-24      ben-cardin   \n",
       "4           5  2015-06-24      neil-young   \n",
       "\n",
       "                                              insult  \\\n",
       "0                                               fool   \n",
       "1                                               DOPE   \n",
       "2                             all talk and no action   \n",
       "3  It's politicians like Cardin that have destroy...   \n",
       "4                                    total hypocrite   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  can you believe this fool, dr. thomas frieden ...   \n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  big time in u.s. today - make america great ag...   \n",
       "3  politician @senatorcardin didn't like that i s...   \n",
       "4  for the nonbeliever, here is a photo of @neily...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [believ, fool, ,, dr., thoma, frieden, cdc, ,,...   \n",
       "1  [believ, fool, ,, dr., thoma, frieden, cdc, ,,...   \n",
       "2  [big, time, u.s., today, -, make, america, gre...   \n",
       "3  [politician, @, senatorcardin, n't, like, said...   \n",
       "4  [nonbeliev, ,, photo, @, neilyoung, offic, $, ...   \n",
       "\n",
       "                                      filtered_tweet  \n",
       "0  believ fool   dr thoma frieden cdc   state    ...  \n",
       "1  believ fool   dr thoma frieden cdc   state    ...  \n",
       "2  big time us today  make america great  politic...  \n",
       "3  politician  senatorcardin nt like said baltimo...  \n",
       "4  nonbeliev   photo  neilyoung offic   request—t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('filtered_Trump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/liuzitian/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('can', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('believe', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('fool', 'NN'),\n",
       " (',', ','),\n",
       " ('dr.', 'NN'),\n",
       " ('thomas', 'VBD'),\n",
       " ('frieden', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('cdc', 'NN'),\n",
       " (',', ','),\n",
       " ('just', 'RB'),\n",
       " ('stated', 'VBN'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('anyone', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('fever', 'NN'),\n",
       " ('should', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('asked', 'VBN'),\n",
       " ('if', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('west', 'NN'),\n",
       " ('africa', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('dope', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "tags = pos_tag(word_tokens[0]) # POS tag the first review (before filtering)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# set vectorizer - CountVectorizer for word counts and TfidfVectorizer for TF-IDF\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# create an array of word counts / TF-IDF scores\n",
    "doc_vec = vectorizer.fit_transform(df.tweet)\n",
    "\n",
    "# convert this to a dataframe\n",
    "df2 = pd.DataFrame(doc_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# set a threshold to drop infrequent words\n",
    "threshold = 0.1\n",
    "\n",
    "# drop words based on the threshold\n",
    "df2 = df2.drop(df2.mean()[df2.mean() < threshold].index.values, axis=1)\n",
    "\n",
    "# join the two datasets together\n",
    "df = df.join(df2, how='left')\n",
    "#df = df.concat(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('filtered_Trump_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# set vectorizer - CountVectorizer for word counts and TfidfVectorizer for TF-IDF\n",
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# create an array of word counts / TF-IDF scores\n",
    "doc_vec = vectorizer.fit_transform(df.tweet)\n",
    "\n",
    "# convert this to a dataframe\n",
    "df2 = pd.DataFrame(doc_vec.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# set a threshold to drop infrequent words\n",
    "threshold = 0.1\n",
    "\n",
    "# drop words based on the threshold\n",
    "df2 = df2.drop(df2.mean()[df2.mean() < threshold].index.values, axis=1)\n",
    "\n",
    "# join the two datasets together\n",
    "df = df.join(df2, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "string = \"I like Liping and Jordan but they are not as good as Michael.\"\n",
    "bigrm = list(nltk.bigrams(string.split())) # split the sentence to words\n",
    "bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# create a single string that combines 200 reviews\n",
    "# words is the list of reviews created earlier\n",
    "long_string = word_tokenize(\",\".join(words[0:200]))\n",
    "\n",
    "# Bigrams\n",
    "finder = BigramCollocationFinder.from_words(long_string)\n",
    "# only bigrams that appear >= 2x\n",
    "finder.apply_freq_filter(2)\n",
    "# return the 10 bigrams with the highest likelihood\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(long_string)\n",
    "# only bigrams that appear >= 2x\n",
    "finder.apply_freq_filter(2)\n",
    "# return the 10 trigrams with the highest likelihood\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install --user textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# calculate polarity and subjectivity\n",
    "df['Polarity'] = df['tweet'].apply(lambda review: TextBlob(str(review)).sentiment.polarity)\n",
    "df['Subjectivity'] = df['tweet'].apply(lambda review: TextBlob(str(review)).sentiment.subjectivity)\n",
    "\n",
    "# sort by polarity (low to high)\n",
    "sorted_df = df.sort_values(by=['Polarity'])\n",
    "\n",
    "# print top 5 positive and negative\n",
    "print(\"Most positive #5 reviews \")\n",
    "print(sorted_df.tweet.tail())\n",
    "print(\"\\n\") # print line break\n",
    "print(\"Most negative #5 reviews \")\n",
    "print(sorted_df.tweet.head())\n",
    "\n",
    "# write the dataframe out to csv\n",
    "df.to_csv(\"filtered_Trump_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"filtered_Trump_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "max_words = 500 # use only the top 500 words\n",
    "k = 10 # set number of topics as 10\n",
    "n_top_words = 20 # print the top 20 words for each topic\n",
    "\n",
    "# helper function to plot topics\n",
    "# see Grisel et al. \n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# vectorise the data into word counts\n",
    "tf_vectorizer = CountVectorizer(max_features=max_words, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "# fit LDA - we'll cover online learning later in the module\n",
    "lda = LDA(n_components=k, max_iter=5, learning_method='online')\n",
    "lda.fit(tf)\n",
    "\n",
    "# get the list of words (feature names)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# print the top words per topic\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
